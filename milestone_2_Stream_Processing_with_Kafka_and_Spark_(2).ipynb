{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AVRO_EventHub_Spark_Processing"
      ],
      "metadata": {
        "id": "dUOAscS7F1Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-eventhub fastavro faker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zTiQ39gEwuW",
        "outputId": "654891f5-bbb7-4c26-fb9c-ff51bd93f317"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-eventhub\n",
            "  Downloading azure_eventhub-5.15.0-py3-none-any.whl.metadata (73 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/73.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.1/73.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-core>=1.27.0 (from azure-eventhub)\n",
            "  Downloading azure_core-1.33.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from azure-eventhub) (4.13.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2025.1.31)\n",
            "Downloading azure_eventhub-5.15.0-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.8/327.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.33.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.1/207.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastavro, faker, azure-core, azure-eventhub\n",
            "Successfully installed azure-core-1.33.0 azure-eventhub-5.15.0 faker-37.1.0 fastavro-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, subprocess\n",
        "\n",
        "spark_version = subprocess.run(\n",
        "    \"curl -s https://downloads.apache.org/spark/ | grep -o 'spark-3\\\\.[0-9]\\\\+\\\\.[0-9]\\\\+' | sort -V | tail -1\",\n",
        "    shell=True, capture_output=True, text=True\n",
        ").stdout.strip()\n",
        "\n",
        "os.environ['SPARK_RELEASE'] = spark_version\n",
        "os.environ['HADOOP_VERSION'] = 'hadoop3'\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['SPARK_HOME'] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/${SPARK_RELEASE}/${SPARK_RELEASE}-bin-hadoop3.tgz\n",
        "!tar xf ${SPARK_RELEASE}-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "hovhihAFF63E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.avro.functions import from_avro\n",
        "from pyspark.sql.functions import col, from_unixtime, to_timestamp, window, session_window\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "w22_OMzSGPvX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"AVRO_Streaming_EventHub\") \\\n",
        "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .config(\"spark.jars.packages\",\n",
        "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
        "            \"org.apache.spark:spark-avro_2.12:3.5.0,\"\n",
        "            \"org.apache.hadoop:hadoop-azure:3.3.1,\"\n",
        "            \"com.microsoft.azure:azure-storage:8.6.6\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "zfhL_GYAH0cl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Azure Blob Storage Key\n",
        "spark.conf.set(\n",
        "    \"fs.azure.account.key.iesstsabbadbaa.blob.core.windows.net\",\n",
        "    \"ZT6z+TYSxF0Xdm0vOCRbIpWoBss2BxOU0EcP2UDceddHX7Kyi8gyJvjyWG5THNp2HOprCHmblb2f+AStp8mAGw==\"\n",
        ")\n",
        "\n",
        "# Event Hub (Kafka API) Settings\n",
        "eventhub_namespace = 'iesstsabbadbaa-grp-06-10'\n",
        "ride_requests_topic = 'ride_request_9'\n",
        "driver_status_topic = 'driver_status_9'\n",
        "connection_string = \"Endpoint=sb://iesstsabbadbaa-grp-06-10.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=XimkJFhlc8Gvorlhh7PRqEZVj0gDkiuDk+AEhH8O9is=\"\n",
        "\n",
        "kafka_common_conf = {\n",
        "    \"kafka.bootstrap.servers\": f\"{eventhub_namespace}.servicebus.windows.net:9093\",\n",
        "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
        "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
        "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";',\n",
        "    \"startingOffsets\": \"latest\",\n",
        "    \"enable.auto.commit\": \"true\",\n",
        "    \"groupIdPrefix\": \"Stream_Analytics_\",\n",
        "    \"auto.commit.interval.ms\": \"5000\"\n",
        "}"
      ],
      "metadata": {
        "id": "YFYYXpFKIGxZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from schemas import ride_request_schema, driver_status_schema\n",
        "\n",
        "ride_request_schema_str = json.dumps(ride_request_schema)\n",
        "driver_status_schema_str = json.dumps(driver_status_schema)\n"
      ],
      "metadata": {
        "id": "GGoXp8ckIbrO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ride = spark.readStream.format(\"kafka\").options(**kafka_common_conf).option(\"subscribe\", ride_requests_topic).load()\n",
        "df_ride = df_ride.select(from_avro(df_ride.value, ride_request_schema_str).alias(\"ride\"))\n",
        "\n",
        "df_ride = df_ride.selectExpr(\n",
        "    \"ride.request_id\", \"ride.user_id\", \"ride.timestamp\",\n",
        "    \"ride.pickup_location.latitude as pickup_lat\", \"ride.pickup_location.longitude as pickup_lon\",\n",
        "    \"ride.destination.latitude as dest_lat\", \"ride.destination.longitude as dest_lon\",\n",
        "    \"ride.status\", \"ride.vehicle_type\", \"ride.estimated_fare\",\n",
        "    \"ride.estimated_duration\", \"ride.estimated_distance\", \"ride.passenger_count\"\n",
        ")"
      ],
      "metadata": {
        "id": "4mPE22CCC1cs",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEMP: Print parsed ride request data to console\n",
        "query_test = df_ride.writeStream.format(\"console\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .start()\n",
        "\n",
        "import time\n",
        "time.sleep(15)\n",
        "\n",
        "query_test.stop()\n"
      ],
      "metadata": {
        "id": "iVnaK5H6zPaB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_driver = spark.readStream.format(\"kafka\").options(**kafka_common_conf).option(\"subscribe\", driver_status_topic).load()\n",
        "df_driver = df_driver.select(from_avro(df_driver.value, driver_status_schema_str).alias(\"driver\"))\n",
        "\n",
        "df_driver = df_driver.selectExpr(\n",
        "    \"driver.driver_id\", \"driver.timestamp\", \"driver.current_location.latitude as latitude\",\n",
        "    \"driver.current_location.longitude as longitude\", \"driver.status\",\n",
        "    \"driver.vehicle_info.vehicle_id as vehicle_id\", \"driver.vehicle_info.vehicle_type as vehicle_type\",\n",
        "    \"driver.vehicle_info.capacity as capacity\", \"driver.current_ride_id\",\n",
        "    \"driver.last_update\", \"driver.battery_level\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "StWc7ChZGoVo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start ride stream to Azure Blob\n",
        "query_ride = df_ride.writeStream.format(\"parquet\") \\\n",
        "    .option(\"path\", \"wasbs://streamed-data-project-group9@iesstsabbadbaa.blob.core.windows.net/output_ride_v2\") \\\n",
        "    .option(\"checkpointLocation\", \"wasbs://streamed-data-project-group9@iesstsabbadbaa.blob.core.windows.net/checkpoints/output_ride_v2\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "# Start driver stream to Azure Blob\n",
        "query_driver = df_driver.writeStream.format(\"parquet\") \\\n",
        "    .option(\"path\", \"wasbs://streamed-data-project-group9@iesstsabbadbaa.blob.core.windows.net/output_driver\") \\\n",
        "    .option(\"checkpointLocation\", \"wasbs://streamed-data-project-group9@iesstsabbadbaa.blob.core.windows.net/checkpoints/output_driver\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .trigger(processingTime=\"5 seconds\") \\\n",
        "    .start()\n",
        "\n",
        "# Let both run for a set duration\n",
        "import time\n",
        "time.sleep(500)\n",
        "\n",
        "# Stop streaming queries gracefully\n",
        "print(\"ðŸ›‘ Stopping active queries...\")\n",
        "for query in spark.streams.active:\n",
        "    query.stop()\n",
        "\n",
        "# Shutdown Spark session\n",
        "spark.stop()\n",
        "print(\"âœ… All done! Data should now be in Azure Blob Storage.\")"
      ],
      "metadata": {
        "id": "mzYqR-s86ar4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}